{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n"
      ],
      "execution_count": 1,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n"
      ],
      "execution_count": 1,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib as mp\n",
        "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "w=1.\n",
        "minx = np.exp(-1/w)\n",
        "miny = np.power(minx,w)*np.log(minx)\n",
        "print('min x,y = <{},{}>'.format(minx,miny))\n",
        "start = minx*0.0001\n",
        "x = [0]+[start * np.power(1.001,i) for i in range(10000)]\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    y=[np.log(xx)*np.power(xx,w) for xx in x]\n",
        "    np.nan_to_num(y, copy=False, nan=0.0)\n",
        "#mp.pyplot.plot(x,y)\n",
        "#mp.pyplot.xscale(\"log\")\n",
        "#mp.pyplot.yscale(\"log\")\n",
        "print('log(1e-60)={}'.format(np.log(1e-60)))\n",
        "print('1e-60*log(1e-60)={}'.format(1e-60*np.log(1e-60)))\n"
      ],
      "execution_count": 1,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/\n",
        "#sklearn.datasets.make_circles(n_samples=100, *, shuffle=True, noise=None, random_state=None, factor=0.8)\n",
        "from sklearn.datasets import make_circles\n",
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame\n",
        "# generate 2d classification dataset\n",
        "X, y = make_circles(n_samples=100, factor=0.5, noise=0.04)\n",
        "y_sparse = [ [1-yy,yy] for yy in y]\n",
        "X_test, y_test = make_circles(n_samples=100, factor=0.5, noise=0.04)\n",
        "y_test_sparse = [ [1-yy,yy] for yy in y_test ]\n",
        "# scatter plot, dots colored by class value\n",
        "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
        "colors = {0:'red', 1:'blue'}\n",
        "fig, ax = pyplot.subplots()\n",
        "grouped = df.groupby('label')\n",
        "for key, group in grouped:\n",
        "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
        "pyplot.show()\n",
        "\n"
      ],
      "execution_count": 1,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "class MyLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,units, **kwargs):\n",
        "    super(MyLayer, self).__init__(kwargs)\n",
        "    w_init = tf.random_normal_initializer()\n",
        "    input_dim = 2\n",
        "    self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
        "                                              dtype='float32'),\n",
        "                         trainable=True)\n",
        "    b_init = tf.zeros_initializer()\n",
        "    self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
        "                                              dtype='float32'),\n",
        "                         trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__(name='my_model')\n",
        "    #self.num_classes = num_classes\n",
        "    # Define your layers here.\n",
        "    #self.myLayer1 = MyLayer(512, activation='relu')\n",
        "    self.myLayer1 = tf.keras.layers.Dense(512)\n",
        "    self.dense_2 = tf.keras.layers.Dense(2)\n",
        "\n",
        "  def call(self, inputs):  # this function is called with operator model.()\n",
        "    # Define your forward pass here,\n",
        "    # using layers you previously defined (in `__init__`).\n",
        "    x = self.myLayer1(inputs)\n",
        "    return self.dense_2(x)\n",
        "\n",
        "model = MyModel()\n",
        "\n",
        "# The compile step specifies the training configuration.\n",
        "#model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "#              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "#              metrics=['accuracy'])\n",
        "\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "#@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = model(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)\n",
        "\n",
        "    \n",
        "#@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = model(images, training=False)\n",
        "\n",
        "EPOCHS=100\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  train_step(X, y_sparse)\n",
        "\n",
        "  test_step(X_test, y_test_sparse)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, Test Loss: {:.3f}, Test Accuracy: {:.3f}'\n",
        "  print(template.format(epoch + 1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result() * 100,\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result() * 100))\n",
        "#  t_loss = loss_object(labels, predictions)\n",
        "#  test_loss(t_loss)\n",
        "#  test_accuracy(labels, predictions)\n",
        "    \n"
      ],
      "execution_count": 1,
      "metadata": {},
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "execution_count": 1,
      "metadata": {},
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}